{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ad21ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException\n",
    "from bs4 import BeautifulSoup\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72122864",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOF while scanning triple-quoted string literal (1162132479.py, line 91)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\U6060174\\AppData\\Local\\Temp\\ipykernel_13688\\1162132479.py\"\u001b[1;36m, line \u001b[1;32m91\u001b[0m\n\u001b[1;33m    driver.quit()\u001b[0m\n\u001b[1;37m                 \n^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
     ]
    }
   ],
   "source": [
    "'''# configure logging\n",
    "logging.basicConfig(filename='error.log', level=logging.ERROR)\n",
    "\n",
    "chrome_path = 'C:\\Program Files (x86)\\Google\\Chrome\\Application\\chrome.exe'\n",
    "df = pd.read_excel(r'C:\\Users\\U6060174\\OneDrive - Clarivate Analytics\\Desktop\\Files\\Jupyter Notebook Work\\US\\Final US Pull & Parse\\input with multiple elements in one go.xlsx')\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "chrome_options.binary_location = chrome_path\n",
    "\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "retry_count = 3  # number of times to retry if an error occurs\n",
    "\n",
    "wait = WebDriverWait(driver, 30)\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    url = row['{URL}']\n",
    "    Application = row['{Application}']\n",
    "    Continuity = row['{Continuity}']\n",
    "    Priority = row['{Priority}']\n",
    "    Attorney = row['{Attorney}']\n",
    "    Assignment = row['{Assignment}']\n",
    "    for _ in range(retry_count):  # retry loop\n",
    "        try:\n",
    "            #driver.delete_all_cookies()\n",
    "            driver.set_page_load_timeout(30)  # set a page load timeout of 30 seconds\n",
    "            driver.get(url)\n",
    "            time.sleep(20) # add a 10 seconds delay\n",
    "            \n",
    "            #Save the First Page            \n",
    "            html1 = driver.page_source\n",
    "            with open(f'C:\\\\Users\\\\U6060174\\\\OneDrive - Clarivate Analytics\\\\Desktop\\\\Files\\\\Jupyter Notebook Work\\\\US\\\\Final US Pull & Parse\\\\Pages\\\\{Application}.html', 'w', encoding='utf-8') as f:\n",
    "                f.write(html1)\n",
    "                time.sleep(10)  # add a 10 seconds delay\n",
    "            \n",
    "            # Click on the first element to save the Second Page\n",
    "            element1 = wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"link_app-data-continuity\"]')))\n",
    "            element1.click()\n",
    "            \n",
    "            time.sleep(10)\n",
    "            html2 = driver.page_source\n",
    "            with open(f'C:\\\\Users\\\\U6060174\\\\OneDrive - Clarivate Analytics\\\\Desktop\\\\Files\\\\Jupyter Notebook Work\\\\US\\\\Final US Pull & Parse\\\\Pages\\\\{Continuity}.html', 'w', encoding='utf-8') as f:\n",
    "                f.write(html2)\n",
    "                time.sleep(10)  # add a 5 second delay before clicking the second element\n",
    "                \n",
    "                \n",
    "            \n",
    "            # Click on the Second element to save the third page\n",
    "            element2 = wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"link_app-data-foriegn-priority\"]')))\n",
    "            element2.click()\n",
    "            \n",
    "            time.sleep(10)\n",
    "            html3 = driver.page_source\n",
    "            with open(f'C:\\\\Users\\\\U6060174\\\\OneDrive - Clarivate Analytics\\\\Desktop\\\\Files\\\\Jupyter Notebook Work\\\\US\\\\Final US Pull & Parse\\\\Pages\\\\{Priority}.html', 'w', encoding='utf-8') as f:\n",
    "                f.write(html3)\n",
    "                time.sleep(10)  # add a 5 seconds delay before clicking the third element\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            # Click on the third element to save the fourth page\n",
    "            element3 = wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"LeftNavLinks\"]/li[8]')))\n",
    "            element3.click()\n",
    "            \n",
    "            time.sleep(10)\n",
    "            html4 = driver.page_source\n",
    "            with open(f'C:\\\\Users\\\\U6060174\\\\OneDrive - Clarivate Analytics\\\\Desktop\\\\Files\\\\Jupyter Notebook Work\\\\US\\\\Final US Pull & Parse\\\\Pages\\\\{Attorney}.html', 'w', encoding='utf-8') as f:\n",
    "                f.write(html4)\n",
    "                time.sleep(10)  # add a 5 seconds delay before clicking the fourth element\n",
    "            \n",
    "            # Click on the fourth element to save the fifth page\n",
    "            element4 = wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"link_app-data-assignments\"]')))\n",
    "            element4.click()\n",
    "            \n",
    "            time.sleep(10)\n",
    "            html5 = driver.page_source\n",
    "            with open(f'C:\\\\Users\\\\U6060174\\\\OneDrive - Clarivate Analytics\\\\Desktop\\\\Files\\\\Jupyter Notebook Work\\\\US\\\\Final US Pull & Parse\\\\Pages\\\\{Assignment}.html', 'w', encoding='utf-8') as f:\n",
    "                f.write(html5)\n",
    "                time.sleep(10)\n",
    "                \n",
    "            break  # break out of the retry loop if successful\n",
    "        except (TimeoutException, WebDriverException) as e:\n",
    "            logging.error(f\"Error occurred while saving the page for {url}: {e}\")\n",
    "            print(f\"Error occurred while saving the page for {url}: {e}\")\n",
    "            time.sleep(10)  # add a 10 seconds delay before retrying\n",
    "    else:  # executed if the retry loop completes without success\n",
    "        logging.error(f\"Failed to save the page after {retry_count} retries for {url}\")\n",
    "        print(f\"Failed to save the page after {retry_count} retries for {url}\")\n",
    "    \n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "329cf326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException\n",
    "import time\n",
    "\n",
    "# configure logging\n",
    "logging.basicConfig(filename='error.log', level=logging.ERROR)\n",
    "\n",
    "chrome_path = 'C:\\Program Files (x86)\\Google\\Chrome\\Application\\chrome.exe'\n",
    "df = pd.read_excel(r'C:\\Users\\U6060174\\OneDrive - Clarivate Analytics\\Desktop\\Files\\Jupyter Notebook Work\\US\\Final US Pull & Parse\\input with multiple elements in one go.xlsx')\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "chrome_options.binary_location = chrome_path\n",
    "\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "retry_count = 3  # number of times to retry if an error occurs\n",
    "\n",
    "wait = WebDriverWait(driver, 30)\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    url = row['{URL}']\n",
    "    Application = row['{Application}']\n",
    "    Continuity = row['{Continuity}']\n",
    "    Priority = row['{Priority}']\n",
    "    Attorney = row['{Attorney}']\n",
    "    Assignment = row['{Assignment}']\n",
    "    for _ in range(retry_count):  # retry loop\n",
    "        try:\n",
    "            driver.set_page_load_timeout(30)  # set a page load timeout of 30 seconds\n",
    "            driver.get(url)\n",
    "            time.sleep(20)\n",
    "\n",
    "            # Save the First Page\n",
    "            html1 = driver.page_source\n",
    "            with open(f'C:\\\\Users\\\\U6060174\\\\OneDrive - Clarivate Analytics\\\\Desktop\\\\Files\\\\Jupyter Notebook Work\\\\US\\\\Final US Pull & Parse\\\\Pages\\\\{Application}.html', 'w', encoding='utf-8') as f:\n",
    "                f.write(html1)\n",
    "                time.sleep(10)\n",
    "\n",
    "            # Click on the first element to save the Second Page\n",
    "            try:\n",
    "                element1 = wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"link_app-data-continuity\"]')))\n",
    "                element1.click()\n",
    "                time.sleep(10)\n",
    "                html2 = driver.page_source\n",
    "                with open(f'C:\\\\Users\\\\U6060174\\\\OneDrive - Clarivate Analytics\\\\Desktop\\\\Files\\\\Jupyter Notebook Work\\\\US\\\\Final US Pull & Parse\\\\Pages\\\\{Continuity}.html', 'w', encoding='utf-8') as f:\n",
    "                    f.write(html2)\n",
    "                    time.sleep(10)\n",
    "            except (TimeoutException, WebDriverException) as e:\n",
    "                logging.error(f\"Error occurred while clicking the first element for {url}: {e}\")\n",
    "                print(f\"Error occurred while clicking the first element for {url}: {e}\")\n",
    "\n",
    "            # Click on the Second element to save the third page\n",
    "            try:\n",
    "                element2 = wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"link_app-data-foriegn-priority\"]')))\n",
    "                element2.click()\n",
    "                time.sleep(10)\n",
    "                html3 = driver.page_source\n",
    "                with open(f'C:\\\\Users\\\\U6060174\\\\OneDrive - Clarivate Analytics\\\\Desktop\\\\Files\\\\Jupyter Notebook Work\\\\US\\\\Final US Pull & Parse\\\\Pages\\\\{Priority}.html', 'w', encoding='utf-8') as f:\n",
    "                    f.write(html3)\n",
    "                    time.sleep(10)\n",
    "            except (TimeoutException, WebDriverException) as e:\n",
    "                logging.error(f\"Error occurred while clicking the second element for {url}: {e}\")\n",
    "                print(f\"Error occurred while clicking the second element for {url}: {e}\")\n",
    "\n",
    "            # Click on the third element to save the fourth page\n",
    "            try:\n",
    "                element3 = wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"LeftNavLinks\"]/li[8]')))\n",
    "                element3.click()\n",
    "                time.sleep(10)\n",
    "                html4 = driver.page_source\n",
    "                with open(f'C:\\\\Users\\\\U6060174\\\\OneDrive - Clarivate Analytics\\\\Desktop\\\\Files\\\\Jupyter Notebook Work\\\\US\\\\Final US Pull & Parse\\\\Pages\\\\{Attorney}.html', 'w', encoding='utf-8') as f:\n",
    "                    f.write(html4)\n",
    "                    time.sleep(10)\n",
    "            except (TimeoutException, WebDriverException) as e:\n",
    "                logging.error(f\"Error occurred while clicking the third element for {url}: {e}\")\n",
    "                print(f\"Error occurred while clicking the third element for {url}: {e}\")\n",
    "\n",
    "            # Click on the fourth element to save the fifth page\n",
    "            try:\n",
    "                element4 = wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"link_app-data-assignments\"]')))\n",
    "                element4.click()\n",
    "                time.sleep(10)\n",
    "                html5 = driver.page_source\n",
    "                with open(f'C:\\\\Users\\\\U6060174\\\\OneDrive - Clarivate Analytics\\\\Desktop\\\\Files\\\\Jupyter Notebook Work\\\\US\\\\Final US Pull & Parse\\\\Pages\\\\{Assignment}.html', 'w', encoding='utf-8') as f:\n",
    "                    f.write(html5)\n",
    "                    time.sleep(10)\n",
    "            except (TimeoutException, WebDriverException) as e:\n",
    "                logging.error(f\"Error occurred while clicking the fourth element for {url}: {e}\")\n",
    "                print(f\"Error occurred while clicking the fourth element for {url}: {e}\")\n",
    "\n",
    "            break  # break out of the retry loop if successful\n",
    "        except (TimeoutException, WebDriverException) as e:\n",
    "            logging.error(f\"Error occurred while saving the page for {url}: {e}\")\n",
    "            print(f\"Error occurred while saving the page for {url}: {e}\")\n",
    "            time.sleep(10)  # add a 10 seconds delay before retrying\n",
    "    else:  # executed if the retry loop completes without success\n",
    "        logging.error(f\"Failed to save the page after {retry_count} retries for {url}\")\n",
    "        print(f\"Failed to save the page after {retry_count} retries for {url}\")\n",
    "\n",
    "driver.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76de144e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Set the path to the Excel file containing the URLs and filenames manually\n",
    "url_file_path = r'C:\\Users\\U6060174\\OneDrive - Clarivate Analytics\\Desktop\\Files\\Jupyter Notebook Work\\US\\Final US Pull & Parse\\input.xlsx'\n",
    "\n",
    "# Read the Excel file into a pandas dataframe\n",
    "df_urls = pd.read_excel(url_file_path)\n",
    "\n",
    "# Create an empty dataframe to store the parsed data\n",
    "column_names = []\n",
    "for page_name in ['Application', 'Continuity', 'Priority', 'Attorney', 'Assignment']:\n",
    "    for suffix in ['Name', 'Data', 'Status']:\n",
    "        column_names.append(f\"{page_name}_{suffix}\")\n",
    "\n",
    "df_data = pd.DataFrame(columns=column_names)\n",
    "\n",
    "# Define the columns to be used in the loop\n",
    "columns = ['{Application}', '{Continuity}', '{Priority}', '{Attorney}', '{Assignment}']\n",
    "\n",
    "# Loop through the URLs in the dataframe and parse the HTML\n",
    "for i, row in df_urls.iterrows():\n",
    "    data_row = {}\n",
    "    for column in columns:\n",
    "        name = row[column]\n",
    "        file_path = f'C:\\\\Users\\\\U6060174\\\\OneDrive - Clarivate Analytics\\\\Desktop\\\\Files\\\\Jupyter Notebook Work\\\\US\\\\Final US Pull & Parse\\\\Pages\\\\{name}.html'\n",
    "        status = 'Success'  # set default status as success\n",
    "\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                html = f.read()\n",
    "\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "            text = soup.get_text(separator='\\n')\n",
    "            text = text.replace('<div>', '<div> ')\n",
    "            text = text.replace('\\n', ' \\n')\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred while processing the file for {name}: {e}\")\n",
    "            text = ''\n",
    "            status = 'Error'  # set status as error if there's an exception\n",
    "\n",
    "        column_prefix = column.strip('{}')\n",
    "        data_row[f\"{column_prefix}_Name\"] = name\n",
    "        data_row[f\"{column_prefix}_Data\"] = text\n",
    "        data_row[f\"{column_prefix}_Status\"] = status\n",
    "\n",
    "    df_data = pd.concat([df_data, pd.DataFrame(data_row, index=[0])])\n",
    "\n",
    "# Write the parsed data to an Excel file\n",
    "output_file_path = r'C:\\Users\\U6060174\\OneDrive - Clarivate Analytics\\Desktop\\Files\\Jupyter Notebook Work\\US\\Final US Pull & Parse\\scraped_data_assignment.xlsx'\n",
    "df_data.to_excel(output_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ad8a49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07344fe6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacaaaad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287661f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bcabf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162ac442",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf95562",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82929400",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949c3ad8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25919544",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43d27a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cfad0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3004e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512b5c89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af757e4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad572ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc55d43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba8cc5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8852269",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
